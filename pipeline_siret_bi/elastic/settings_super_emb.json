{"mappings": {"properties": {"nic": {"type": "text", "fields": {"keyword": {"type": "keyword", "ignore_above": 256}}},"embedding": {
      "type": "dense_vector",
      "dims": 128
    } "sirus_id": {"type": "text", "fields": {"keyword": {"type": "keyword", "ignore_above": 256}}}}}, "settings": {"index": {"refresh_interval": "5s", "number_of_shards": "2", "analysis": {"filter": {"french_stop": {"type": "stop", "stopwords": "_french_"}, "acronymizer": {"pattern": "(?<=\\w)[A-z]*(\\s|\b|\\-)", "replace": "", "type": "pattern_replace"}, "french_elision": {"type": "elision", "articles": ["l", "m", "t", "qu", "n", "s", "j", "d", "c"], "articles_case": "True"}, "trunc5": {"length": "5", "type": "truncate"}, "trunc4": {"length": "4", "type": "truncate"}, "trunc3": {"length": "3", "type": "truncate"}, "trunc2": {"length": "2", "type": "truncate"}, "cleansigle": {"pattern": "\\.|\\-|\\s*", "replace": "", "type": "pattern_replace"}, "synonyms_rs": {"type": "synonym_graph", "synonyms_path": "analysis/synonymes.txt"}, "french_stemmer": {"type": "stemmer", "language": "light_french"}, "pattern_stop": {"pattern": "\ble\b|\bla\b|\bles\b|\bde\b|\bdes\b|\bdu\b|\ben\b|l'|d'", "replace": "", "type": "pattern_replace"}, "toklen3": {"type": "length", "min": "3"}, "french_keywords": {"keywords": ["Exemple"], "type": "keyword_marker"}}, "analyzer": {"ngram_analyzer": {"filter": "lowercase", "tokenizer": "ngram_tokenizer"}, "acronym": {"filter": ["asciifolding", "lowercase", "pattern_stop", "acronymizer", "cleansigle", "toklen3"], "type": "custom", "tokenizer": "keyword"}, "search_syn_rs": {"filter": ["asciifolding", "french_elision", "lowercase", "synonyms_rs", "french_stop", "french_stemmer"], "tokenizer": "letter"}, "trunc5": {"filter": "trunc5", "tokenizer": "keyword"}, "trunc4": {"filter": "trunc4", "tokenizer": "keyword"}, "stemming": {"filter": ["asciifolding", "french_elision", "lowercase", "french_stop", "french_stemmer", "unique"], "tokenizer": "letter"}, "trunc3": {"filter": "trunc3", "tokenizer": "keyword"}, "trunc2": {"filter": "trunc2", "tokenizer": "keyword"}, "sigle": {"filter": ["classic"], "type": "custom", "tokenizer": "keyword"}}, "tokenizer": {"ngram_tokenizer": {"token_chars": ["letter", "digit"], "min_gram": "2", "type": "edge_ngram", "max_gram": "8"}}}, "number_of_replicas": "2"}}}