{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1er Modèle\n",
    "\n",
    "Author : bsanchez@starclay.fr\n",
    "date : 20/07/2020\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import os\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn import tree\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import scikitplot as skplt\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5547cee964b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m#nltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m#stop-words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# data visualisation and manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "#configure\n",
    "# sets matplotlib to inline and displays graphs below the corressponding cell.\n",
    "%matplotlib inline  \n",
    "style.use('fivethirtyeight')\n",
    "sns.set(style='whitegrid',color_codes=True)\n",
    "\n",
    "#nltk\n",
    "import nltk\n",
    "\n",
    "#stop-words\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stop_words=set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# tokenizing\n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "\n",
    "#keras\n",
    "import keras\n",
    "from keras.preprocessing.text import one_hot,Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense , Flatten ,Embedding,Input\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text_1 = \"groupe pomona\"\n",
    "sample_text_2 = \"education national\"\n",
    "sample_text_3 = \"gendarmerie national\"\n",
    "sample_text_4 = \"manpower\"\n",
    "sample_text_5 = \"Hopital teunon paris\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corp=[sample_text_1,sample_text_2,sample_text_3,sample_text_4,sample_text_5]\n",
    "no_docs=len(corp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['groupe pomona',\n",
       " 'education national',\n",
       " 'gendarmerie national',\n",
       " 'manpower',\n",
       " 'Hopital teunon paris']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The encoding for document 1  is :  [7, 5]\n",
      "The encoding for document 2  is :  [10, 27]\n",
      "The encoding for document 3  is :  [20, 27]\n",
      "The encoding for document 4  is :  [33]\n",
      "The encoding for document 5  is :  [9, 13, 24]\n"
     ]
    }
   ],
   "source": [
    "vocab_size=50 \n",
    "encod_corp=[]\n",
    "for i,doc in enumerate(corp):\n",
    "    encod_corp.append(one_hot(doc,50))\n",
    "    print(\"The encoding for document\",i+1,\" is : \", one_hot(doc,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7, 5], [10, 27], [20, 27], [33], [9, 13, 24]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encod_corp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum number of words in any document is :  3\n"
     ]
    }
   ],
   "source": [
    "maxlen = -1\n",
    "for doc in corp:\n",
    "    tokens=nltk.word_tokenize(doc)\n",
    "    if(maxlen<len(tokens)):\n",
    "        maxlen=len(tokens)\n",
    "print(\"The maximum number of words in any document is : \", maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "a = pd.DataFrame([[[\"a\",\"a\"],[\"a\"]],[[\"a\",\"a\"],[\"a\"]],[[\"a\",\"a\"],[\"a\"]]],columns=[\"za\",\"ba\"])\n",
    "# z = [a[col] for col in a]\n",
    "np.asarray(a[\"za\"].values.tolist()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7,  5,  0],\n",
       "       [10, 27,  0],\n",
       "       [20, 27,  0],\n",
       "       [33,  0,  0],\n",
       "       [ 9, 13, 24]], dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_corp = pad_sequences(encod_corp,maxlen=maxlen,padding='post',value=0.0)\n",
    "pad_corp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The padded encoding for document 1  is :  [7 5 0]\n",
      "The padded encoding for document 2  is :  [10 27  0]\n",
      "The padded encoding for document 3  is :  [20 27  0]\n",
      "The padded encoding for document 4  is :  [33  0  0]\n",
      "The padded encoding for document 5  is :  [ 9 13 24]\n"
     ]
    }
   ],
   "source": [
    "for i,doc in enumerate(pad_corp):\n",
    "     print(\"The padded encoding for document\",i+1,\" is : \",doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input_1:0\", shape=(None, 5, 3), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "input = Input(shape=(no_docs,maxlen),dtype='float64')\n",
    "print(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_input = Input(shape=(maxlen,),dtype='float64')  \n",
    "\n",
    "# creating the embedding\n",
    "word_embedding = Embedding(input_dim=vocab_size\n",
    "                         ,output_dim=8\n",
    "                         ,input_length=maxlen)(word_input)\n",
    "\n",
    "word_vec = Flatten()(word_embedding) # flatten\n",
    "embed_model = Model([word_input],word_vec) # combining all into a Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model.compile(optimizer=keras.optimizers.Adam(lr=1e-3),loss='binary_crossentropy',metrics=['acc']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "Tensor(\"embedding/embedding_lookup/Identity_1:0\", shape=(None, 3, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(type(word_embedding))\n",
    "print(word_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 3)]               0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 3, 8)              400       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 24)                0         \n",
      "=================================================================\n",
      "Total params: 400\n",
      "Trainable params: 400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(embed_model.summary()) # summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=embed_model.predict(pad_corp) # finally getting the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embeddings :  (5, 24)\n",
      "[[-5.3673498e-03 -1.8788934e-02 -4.4557441e-02 -2.6957249e-02\n",
      "  -6.2922612e-03 -1.4181305e-02 -3.0416477e-02 -2.8175617e-02\n",
      "   1.7932821e-02  1.7127883e-02  3.8691927e-02 -1.8712878e-02\n",
      "   3.8859393e-02 -3.1095792e-02  2.3134384e-02 -2.9454781e-02\n",
      "   4.0332008e-02 -3.6765348e-02  4.9045954e-02 -4.7364235e-03\n",
      "  -2.8743673e-02 -4.3033481e-02  2.7902950e-02 -3.7243377e-02]\n",
      " [-1.7941654e-02 -3.5849582e-02  1.7087534e-04 -3.2103911e-02\n",
      "  -1.5646923e-02  2.7943339e-02 -4.7287717e-03 -2.8392745e-02\n",
      "  -1.7254472e-02  1.9911043e-03 -1.3535045e-02  1.8280435e-02\n",
      "  -1.4121603e-02 -6.5667555e-04  6.1333887e-03 -3.4374021e-02\n",
      "   4.0332008e-02 -3.6765348e-02  4.9045954e-02 -4.7364235e-03\n",
      "  -2.8743673e-02 -4.3033481e-02  2.7902950e-02 -3.7243377e-02]\n",
      " [-2.2350682e-02 -1.5841924e-02 -3.5081852e-02  4.5776252e-02\n",
      "  -2.3162497e-02 -4.8936989e-02 -8.7780952e-03 -2.6657630e-02\n",
      "  -1.7254472e-02  1.9911043e-03 -1.3535045e-02  1.8280435e-02\n",
      "  -1.4121603e-02 -6.5667555e-04  6.1333887e-03 -3.4374021e-02\n",
      "   4.0332008e-02 -3.6765348e-02  4.9045954e-02 -4.7364235e-03\n",
      "  -2.8743673e-02 -4.3033481e-02  2.7902950e-02 -3.7243377e-02]\n",
      " [ 1.6642097e-02  2.5063459e-02  2.7626976e-03 -2.2484768e-02\n",
      "   4.0501129e-02 -3.4424387e-02  9.2756003e-05 -3.7266754e-02\n",
      "   4.0332008e-02 -3.6765348e-02  4.9045954e-02 -4.7364235e-03\n",
      "  -2.8743673e-02 -4.3033481e-02  2.7902950e-02 -3.7243377e-02\n",
      "   4.0332008e-02 -3.6765348e-02  4.9045954e-02 -4.7364235e-03\n",
      "  -2.8743673e-02 -4.3033481e-02  2.7902950e-02 -3.7243377e-02]\n",
      " [-3.5800815e-02  2.7670372e-02  3.8695183e-02  4.6699531e-03\n",
      "  -3.5101175e-04 -3.7364043e-02  4.4997659e-02 -2.8655922e-02\n",
      "  -1.9754315e-02 -3.1789266e-02  1.0385238e-02  2.7115095e-02\n",
      "  -4.4013038e-03 -2.4900461e-02  3.6952008e-02 -1.7491937e-02\n",
      "  -1.0704838e-02  1.0661088e-02 -8.4030405e-03  7.4628964e-03\n",
      "  -3.5837665e-03 -1.4643170e-02  3.3019427e-02 -1.0159671e-02]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of embeddings : \",embeddings.shape)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 24)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04033201, -0.03676535,  0.04904595, -0.00473642, -0.02874367,\n",
       "        -0.04303348,  0.02790295, -0.03724338,  0.04961969, -0.02003281,\n",
       "        -0.04558759,  0.01380639,  0.03038007, -0.0179113 , -0.01952933,\n",
       "        -0.0267565 , -0.0331312 , -0.00247995, -0.01352979, -0.03143342,\n",
       "         0.00326723,  0.00689859, -0.0202565 ,  0.03883375],\n",
       "       [ 0.04982357,  0.04128337, -0.01227608, -0.02850893,  0.01289573,\n",
       "        -0.01466572,  0.04804214,  0.04450727, -0.03580081,  0.02767037,\n",
       "         0.03869518,  0.00466995, -0.00035101, -0.03736404,  0.04499766,\n",
       "        -0.02865592,  0.04033201, -0.03676535,  0.04904595, -0.00473642,\n",
       "        -0.02874367, -0.04303348,  0.02790295, -0.03724338]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_model.predict([[0,26,2],[12,9,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "a = DictVectorizer(sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>un</th>\n",
       "      <th>deux</th>\n",
       "      <th>bar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>d</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a</td>\n",
       "      <td>f</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  un deux  bar\n",
       "0  a    a    0\n",
       "1  a    c    1\n",
       "2  a    a    0\n",
       "3  a    d    2\n",
       "4  a    f    3\n",
       "5  a    c    1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "y = [0,1,0,0,0,0]\n",
    "a = pd.DataFrame([[\"a\",\"a\"],[\"a\",\"c\"],[\"a\",\"a\"],[\"a\",\"d\"],[\"a\",\"f\"],[\"a\",\"c\"]],columns=[\"un\",\"deux\"])\n",
    "a['bar'] = +a['un']+' '+a['deux']\n",
    "le = preprocessing.LabelEncoder()\n",
    "a['bar'] = le.fit_transform(a['bar'])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(a, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>un</th>\n",
       "      <th>deux</th>\n",
       "      <th>bar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a</td>\n",
       "      <td>f</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>d</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  un deux  bar\n",
       "5  a    c    1\n",
       "2  a    a    0\n",
       "4  a    f    3\n",
       "3  a    d    2"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "# LSTM EMBEDDING NO PADDING\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, TimeDistributed\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.asarray([\n",
    "     tf.convert_to_tensor(np.asarray([10,5,7]).astype(np.float32))\n",
    "    ,tf.convert_to_tensor(np.asarray([8,16]).astype(np.float32))\n",
    "    ,tf.convert_to_tensor(np.asarray([2,3]).astype(np.float32))\n",
    "    ,tf.convert_to_tensor(np.asarray([1,2]).astype(np.float32))\n",
    "    ,tf.convert_to_tensor(np.asarray([9,9]).astype(np.float32))\n",
    "])\n",
    "labels = [1,1,2,3,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LSTM' object has no attribute 'inputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-8654fa84a606>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LSTM' object has no attribute 'inputs'"
     ]
    }
   ],
   "source": [
    "inp = Input(shape=(None, 20)) \n",
    "lstm = tf.keras.layers.LSTM(4)\n",
    "lstm.inputs(inp)\n",
    "output = lstm(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(data,labels,  verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = tf.keras.layers.LSTM(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer lstm_5 is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: [1, 3]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-37bb3d6386d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 663\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    664\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m     \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    980\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 982\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2616\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2617\u001b[0m       input_spec.assert_input_compatibility(\n\u001b[0;32m-> 2618\u001b[0;31m           self.input_spec, inputs, self.name)\n\u001b[0m\u001b[1;32m   2619\u001b[0m       \u001b[0minput_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2620\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0minput_list\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    178\u001b[0m                          \u001b[0;34m'expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                          \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'. Full shape received: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                          str(x.shape.as_list()))\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer lstm_5 is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: [1, 3]"
     ]
    }
   ],
   "source": [
    "output = lstm(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "# FASTTEXT\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution() # right after `import tensorflow as tf` : MEMORY LEAK \n",
    "\n",
    "# data visualisation and manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "#configure\n",
    "# sets matplotlib to inline and displays graphs below the corressponding cell.\n",
    "%matplotlib inline  \n",
    "style.use('fivethirtyeight')\n",
    "sns.set(style='whitegrid',color_codes=True)\n",
    "\n",
    "#nltk\n",
    "import nltk\n",
    "\n",
    "#stop-words\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stop_words=set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# tokenizing\n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "\n",
    "#keras\n",
    "import keras\n",
    "from keras.preprocessing.text import one_hot,Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Embedding, Input, LSTM\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "ft_model = load_model('cc.fr.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = ft_model.get_dimension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = [\n",
    "    \"bateau chien\"\n",
    "    ,\"berger mouton chien\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size 5 (filtré) vs 4\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "dico_vocab = Dictionary() \n",
    "list_len_largest_token = -1\n",
    "encoded_corpus = []\n",
    "nb_docs = len(my_data)\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]                \n",
    "list_len_largest_token = -1\n",
    "                \n",
    "fdist = FreqDist()\n",
    "                \n",
    "#Determiner la plus grande list de token \n",
    "for index_doc, doc in enumerate(my_data):\n",
    "    tokens = str(doc).split(\" \")\n",
    "    dico_vocab.add_documents([tokens])\n",
    "    for token in tokens:\n",
    "        fdist[token.lower()] += 1\n",
    "    if(list_len_largest_token < len(tokens)):\n",
    "        list_len_largest_token = len(tokens)\n",
    "\n",
    "more_than_one = list(filter(lambda x: x[1] >= 2, fdist.items()))\n",
    "\n",
    "vocab_size = len(dico_vocab) + 1\n",
    "\n",
    "print(f\"vocab_size {vocab_size} (filtré) vs {len(dico_vocab)}\")\n",
    "\n",
    "print(list_len_largest_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index_doc, doc in enumerate(my_data):\n",
    "    tokens = doc.split(\" \")\n",
    "    my_data[index_doc] = tokens\n",
    "tokenizer = Tokenizer(num_words=vocab_size, lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(my_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 1], [3, 4, 1]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_seq_train = tokenizer.texts_to_sequences(my_data)\n",
    "word_seq_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(len(word_seq_train)):\n",
    "    word_seq_train[x] = flatten(pad_sequences([word_seq_train[x]]\n",
    "                                                          , maxlen = 5\n",
    "                                                          , padding = 'post'\n",
    "                                                          , value = 0.0).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "number of null word embeddings: 1\n"
     ]
    }
   ],
   "source": [
    "words_not_found = []\n",
    "embed_dim = 300\n",
    "embedding_matrix = np.zeros((vocab_size, embed_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    print(i)\n",
    "    embedding_vector = ft_model[word]\n",
    "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        words_not_found.append(word)\n",
    "print(f'number of null word embeddings: {np.sum(np.sum(embedding_matrix, axis=1) == 0)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 1, 0, 0, 0], [3, 4, 1, 0, 0]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_seq_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model\n",
    "model = Sequential()\n",
    "# emmbed word vectors\n",
    "model.add(Embedding(vocab_size\n",
    "                    ,300\n",
    "                    , weights = [embedding_matrix]\n",
    "                    , trainable = False\n",
    "                    , mask_zero = True ))\n",
    "# learn the correlations\n",
    "\n",
    "model.add(LSTM(256,return_sequences=False))\n",
    "\n",
    "model.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_col_vector = model.predict(np.array(word_seq_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 256)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_col_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-7.89595651e-04, -7.37569109e-03,  2.08314005e-02,\n",
       "        -1.73259098e-02,  1.71414260e-02, -5.90225682e-03,\n",
       "         6.87512057e-03,  1.80747602e-02,  2.17018393e-03,\n",
       "        -1.96348373e-02, -1.64068528e-02,  1.95564907e-02,\n",
       "         1.39490496e-02,  1.63350031e-02,  3.45415995e-03,\n",
       "        -6.01316860e-04, -1.36102876e-03, -9.70504899e-03,\n",
       "        -4.19419585e-03,  6.98120566e-03,  3.29457433e-03,\n",
       "        -1.02660814e-02,  2.48544998e-02, -8.49526282e-03,\n",
       "        -2.00961111e-03, -2.67344713e-02, -7.27819139e-03,\n",
       "        -1.72056891e-02,  1.76884308e-02, -6.51207101e-03,\n",
       "         2.20525754e-03, -7.55632718e-05,  2.14732271e-02,\n",
       "         1.47005112e-03, -1.24273049e-02,  1.34349950e-02,\n",
       "         1.27875777e-02,  9.53956158e-04, -2.79732514e-03,\n",
       "        -4.61942097e-03,  7.19183311e-03, -7.64042046e-03,\n",
       "         1.11569688e-02,  3.12791346e-03, -4.70059877e-03,\n",
       "         2.14979425e-02, -1.72848441e-02,  5.89560485e-03,\n",
       "        -1.58366766e-02, -1.47881582e-02,  1.05014220e-02,\n",
       "        -1.88693888e-02,  2.99350329e-04,  1.24797868e-02,\n",
       "        -2.25396856e-04,  8.06882512e-03,  1.91428680e-02,\n",
       "         7.78287416e-03,  8.75564665e-03,  1.91183784e-03,\n",
       "        -2.01798733e-02,  1.02696382e-02,  3.53272781e-02,\n",
       "        -8.65275133e-03,  1.43402014e-02, -1.46575980e-02,\n",
       "        -8.99324100e-03,  6.94766361e-03,  7.10076699e-03,\n",
       "        -4.63075237e-03,  2.33054496e-02,  2.92820148e-02,\n",
       "         1.09700719e-02,  4.06957790e-03,  1.53114740e-03,\n",
       "         2.79405043e-02,  2.14420841e-03, -2.47831340e-03,\n",
       "         2.81655462e-03, -5.18418290e-03, -2.81557254e-03,\n",
       "         6.54415181e-03, -5.73375449e-03, -9.50386841e-03,\n",
       "        -9.34881717e-03, -1.57101713e-02,  1.15246847e-02,\n",
       "        -5.69667388e-03,  1.76711543e-03,  9.74960998e-03,\n",
       "        -1.01841036e-02, -1.27755841e-02,  5.41067123e-03,\n",
       "         5.77236339e-03,  4.12950385e-03,  1.38871307e-02,\n",
       "         5.29813254e-03,  2.80749216e-03, -2.70056818e-03,\n",
       "        -1.14996079e-02,  1.32013438e-02,  2.49149837e-03,\n",
       "         5.62702166e-03, -1.69902109e-02,  1.23324702e-02,\n",
       "        -1.32129518e-02, -1.71428695e-02,  2.08914205e-02,\n",
       "         1.71758002e-03,  1.50206883e-03, -3.58380703e-03,\n",
       "        -2.81568337e-02,  4.41349000e-02, -2.17194110e-03,\n",
       "        -8.96339305e-03, -1.40463663e-02,  2.86005009e-02,\n",
       "        -1.52879022e-02,  2.32920069e-02,  1.28234708e-04,\n",
       "         1.14541203e-02, -1.97739690e-03, -5.72163984e-03,\n",
       "         1.69613529e-02,  2.23771986e-02, -1.68598648e-02,\n",
       "         8.79884046e-03, -1.45956343e-02,  6.88159373e-03,\n",
       "         6.24686666e-03,  4.87116352e-03,  3.14293653e-02,\n",
       "         2.21333262e-02, -5.89168340e-04,  1.76486140e-03,\n",
       "        -1.68508361e-03,  1.91159006e-02, -1.30001083e-02,\n",
       "        -1.35754107e-03, -8.94767605e-03,  8.42454098e-03,\n",
       "         5.29700611e-03,  2.62653851e-03,  1.84472743e-02,\n",
       "        -4.86729061e-03,  4.22349572e-03, -2.86317561e-02,\n",
       "         1.59704015e-02, -6.33107556e-04, -9.11298266e-04,\n",
       "        -4.20203945e-03,  2.96025388e-02, -1.04809608e-02,\n",
       "        -1.65947564e-02, -3.22541874e-03,  2.22776439e-02,\n",
       "         1.16257381e-03, -5.58860879e-03, -2.52106925e-04,\n",
       "        -1.12409946e-02, -1.42402817e-02,  3.57494433e-03,\n",
       "         6.63544796e-03, -2.60251835e-02, -8.83771293e-03,\n",
       "        -1.37923704e-02,  1.55700976e-03, -1.39449956e-02,\n",
       "         2.59583630e-03, -8.13877583e-03, -1.18219331e-02,\n",
       "        -3.43221449e-03,  1.38894161e-02,  2.81043234e-03,\n",
       "         1.70219783e-02, -4.27858904e-03, -9.94837098e-03,\n",
       "        -3.58698936e-03,  1.45905605e-02, -1.59580968e-02,\n",
       "        -3.80631373e-03, -5.74980164e-03,  1.96657572e-02,\n",
       "         1.67907421e-02, -7.01964227e-03,  1.31312786e-02,\n",
       "         1.59585872e-03,  3.16867558e-03,  1.98212964e-03,\n",
       "        -2.02863142e-04, -1.66065264e-02,  1.15420837e-02,\n",
       "         1.27896685e-02, -1.83485541e-02, -6.14068063e-04,\n",
       "        -1.80582870e-02,  9.43594053e-03,  7.84244575e-03,\n",
       "         1.23864161e-02,  2.29866803e-03,  1.33857327e-02,\n",
       "        -5.22043183e-03,  8.05832166e-03, -2.51923595e-02,\n",
       "        -1.41915381e-02, -1.13819968e-02, -2.03868514e-03,\n",
       "         1.27731720e-02,  1.67604350e-02,  1.34801818e-03,\n",
       "         6.36774814e-03, -3.28303035e-03, -2.51852581e-03,\n",
       "         2.34510121e-03, -1.18608493e-02,  8.59955791e-03,\n",
       "        -3.41780521e-02,  9.82322916e-03, -3.06943990e-02,\n",
       "        -1.75725464e-02,  1.84276998e-02,  1.54871680e-02,\n",
       "         1.76941212e-02, -2.36492977e-03,  5.50269429e-03,\n",
       "         9.94941592e-03,  1.21729132e-02, -2.60030907e-02,\n",
       "         8.80523678e-03, -1.70184914e-02,  1.59748420e-02,\n",
       "         1.73164718e-02,  1.15536386e-02,  1.24429269e-02,\n",
       "        -4.91306419e-03, -1.25530496e-04, -7.67558208e-03,\n",
       "        -8.12873710e-03,  6.62431354e-04, -2.32471302e-02,\n",
       "        -2.99547762e-02, -1.22121349e-02, -1.52040944e-02,\n",
       "        -5.74064907e-03,  1.03106722e-02,  1.08133592e-02,\n",
       "        -5.18176286e-03,  6.98101521e-03, -3.16058565e-03,\n",
       "         7.11381761e-03, -1.20509358e-03,  8.05048179e-03,\n",
       "        -4.22267523e-03, -3.55666591e-04,  1.66158117e-02,\n",
       "         8.56450945e-03],\n",
       "       [ 2.21079551e-02, -1.80775505e-02,  6.37906091e-03,\n",
       "        -1.34907225e-02,  1.48024531e-02, -7.31257256e-03,\n",
       "         7.83887599e-03,  3.12307347e-02,  1.64831337e-02,\n",
       "        -2.33513489e-02, -2.59409584e-02,  1.16356751e-02,\n",
       "         2.26903185e-02,  2.51026247e-02,  7.65365548e-03,\n",
       "        -6.33462751e-03, -1.20659852e-02, -1.02984421e-02,\n",
       "        -2.70214640e-02,  1.20221712e-02,  7.94861745e-03,\n",
       "         1.27606457e-02,  1.59814097e-02, -1.27181201e-03,\n",
       "         1.03055807e-02, -4.34602797e-02, -9.96939652e-03,\n",
       "        -1.41740283e-02,  2.21573934e-02, -2.77499910e-02,\n",
       "         1.63406488e-02, -1.09948795e-02,  2.82373931e-02,\n",
       "        -5.00199094e-04, -1.21858763e-02,  1.11196632e-03,\n",
       "         2.18166374e-02,  7.54407234e-03,  3.10343201e-03,\n",
       "         1.30759170e-02,  1.38099946e-03, -2.37892997e-02,\n",
       "         1.12760970e-02, -1.04802046e-02, -2.10208073e-03,\n",
       "         2.50801444e-02, -2.31113285e-02,  3.53717594e-03,\n",
       "        -1.44901155e-02, -1.12197110e-02,  1.45153785e-02,\n",
       "        -7.60397268e-03, -1.02116140e-02,  1.60157811e-02,\n",
       "         2.45734671e-04,  1.20682884e-02,  3.18610743e-02,\n",
       "         1.33541701e-02,  5.65909175e-03, -5.50853275e-03,\n",
       "        -6.51268009e-03, -3.55047290e-03,  5.19436784e-02,\n",
       "        -1.34239281e-02, -1.74585875e-04, -1.76749993e-02,\n",
       "        -1.62950512e-02, -1.09072411e-02,  7.53307482e-03,\n",
       "        -1.71731133e-02,  3.09347156e-02,  3.91020440e-02,\n",
       "         1.30211189e-02, -5.75714375e-06,  1.60973538e-02,\n",
       "         1.54649457e-02,  1.03884644e-03, -2.83868797e-02,\n",
       "        -1.07257944e-02, -8.45322851e-04, -2.94320309e-03,\n",
       "         4.51650517e-03, -1.84669998e-02, -1.85478609e-02,\n",
       "        -2.06718198e-03, -2.58692261e-02,  8.96357279e-03,\n",
       "         6.91828504e-03, -6.10304717e-03,  1.13280816e-02,\n",
       "        -1.33499699e-02, -1.57040097e-02,  6.15679007e-03,\n",
       "         9.91647225e-03,  2.62754075e-02,  3.06744240e-02,\n",
       "         1.48903830e-02, -4.63463087e-03, -2.21519582e-02,\n",
       "        -1.06347017e-02,  1.03206569e-02,  4.16119816e-04,\n",
       "         3.70593159e-03, -1.09752696e-02,  4.76837205e-03,\n",
       "        -6.30964618e-03, -2.54301615e-02,  4.09004875e-02,\n",
       "         6.47595990e-03,  2.74129631e-03,  6.30955352e-03,\n",
       "        -3.80857997e-02,  3.64053547e-02,  1.18867215e-02,\n",
       "        -2.24294178e-02, -3.45944948e-02,  3.69646773e-02,\n",
       "        -1.92812476e-02,  3.06001734e-02,  1.68806349e-03,\n",
       "         2.37420015e-02,  1.80522248e-03, -2.19476788e-04,\n",
       "         3.29695456e-02,  6.34386484e-03, -9.15173814e-03,\n",
       "         6.17828686e-03, -6.56809751e-03,  2.54775733e-02,\n",
       "        -5.73036401e-03,  9.80871078e-03,  1.42593700e-02,\n",
       "         2.59376243e-02, -2.96818186e-03, -2.89870985e-03,\n",
       "        -5.56930061e-03,  3.05103846e-02, -2.54682973e-02,\n",
       "        -1.73395369e-02, -1.59480125e-02,  7.70309195e-03,\n",
       "         3.69878416e-03,  1.17508406e-02,  3.18798199e-02,\n",
       "        -1.45177590e-02,  1.93184260e-02, -4.61903587e-02,\n",
       "         3.08603365e-02, -9.77767166e-03, -2.23869998e-02,\n",
       "         5.50109136e-04,  3.76434848e-02, -4.26510302e-03,\n",
       "        -1.16778361e-02,  1.83056074e-03,  1.09045841e-02,\n",
       "         7.35088345e-03, -6.44543767e-03,  6.88500004e-03,\n",
       "        -1.85897555e-02, -1.36702163e-02,  1.47964153e-02,\n",
       "        -2.32208357e-03, -2.08734982e-02, -2.16644946e-02,\n",
       "        -4.24258374e-02,  8.39173794e-03, -2.42717303e-02,\n",
       "         4.52430826e-03, -2.81931320e-03, -7.98230991e-03,\n",
       "         5.39858872e-03,  2.72646565e-02,  3.67195718e-03,\n",
       "         1.59337390e-02,  2.38831807e-03, -7.28753675e-03,\n",
       "        -1.30820731e-02,  1.35610988e-02, -2.77492497e-02,\n",
       "        -7.10715679e-03, -9.26468149e-03,  3.65882367e-02,\n",
       "         2.28190478e-02, -1.34485401e-02,  2.21239291e-02,\n",
       "         1.13093816e-02, -5.34023996e-03,  7.21934298e-03,\n",
       "        -2.34856969e-03, -4.86261956e-03,  1.52492216e-02,\n",
       "         9.56754107e-03, -3.61621566e-02,  1.02686491e-02,\n",
       "        -2.19792314e-02, -1.22175850e-02,  1.11446744e-02,\n",
       "         2.02709921e-02,  5.16695902e-03,  1.50694884e-02,\n",
       "        -1.25176925e-02,  4.89096157e-03, -3.11836544e-02,\n",
       "        -1.26364259e-02, -1.12009291e-02, -2.88397633e-03,\n",
       "         1.21441286e-03,  1.85081735e-02,  7.34461180e-04,\n",
       "        -1.19834254e-02, -1.99452080e-02,  2.72110803e-03,\n",
       "         8.01188033e-03, -7.38879992e-03,  7.17239128e-03,\n",
       "        -3.15618962e-02,  5.11205662e-03, -1.20633338e-02,\n",
       "        -7.74942338e-03,  2.07552835e-02,  1.19651593e-02,\n",
       "         3.13407034e-02,  1.36786059e-03,  8.64642207e-03,\n",
       "         3.50121409e-02,  1.32626947e-02, -2.38176230e-02,\n",
       "        -1.61724072e-02, -2.21535210e-02,  4.97100409e-03,\n",
       "         3.86859514e-02, -4.34587849e-03,  1.90100935e-03,\n",
       "         1.64963363e-03,  5.43489214e-03, -1.86602604e-02,\n",
       "        -1.60942748e-02,  9.38689802e-03, -8.71696323e-03,\n",
       "        -3.60639952e-02, -9.86954104e-03, -3.02454159e-02,\n",
       "         6.85044331e-03,  1.72549654e-02,  9.18768253e-03,\n",
       "        -7.16706365e-03,  1.69102754e-02, -4.66912752e-03,\n",
       "         3.39231752e-02,  3.37922294e-03,  2.55926512e-02,\n",
       "         5.52145950e-03, -4.87248925e-03,  2.41219997e-02,\n",
       "         6.77003153e-03]], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_col_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['accuracy',\n",
       " 'adjusted_mutual_info_score',\n",
       " 'adjusted_rand_score',\n",
       " 'average_precision',\n",
       " 'balanced_accuracy',\n",
       " 'brier_score_loss',\n",
       " 'completeness_score',\n",
       " 'explained_variance',\n",
       " 'f1',\n",
       " 'f1_macro',\n",
       " 'f1_micro',\n",
       " 'f1_samples',\n",
       " 'f1_weighted',\n",
       " 'fowlkes_mallows_score',\n",
       " 'homogeneity_score',\n",
       " 'jaccard',\n",
       " 'jaccard_macro',\n",
       " 'jaccard_micro',\n",
       " 'jaccard_samples',\n",
       " 'jaccard_weighted',\n",
       " 'max_error',\n",
       " 'mutual_info_score',\n",
       " 'neg_log_loss',\n",
       " 'neg_mean_absolute_error',\n",
       " 'neg_mean_squared_error',\n",
       " 'neg_mean_squared_log_error',\n",
       " 'neg_median_absolute_error',\n",
       " 'normalized_mutual_info_score',\n",
       " 'precision',\n",
       " 'precision_macro',\n",
       " 'precision_micro',\n",
       " 'precision_samples',\n",
       " 'precision_weighted',\n",
       " 'r2',\n",
       " 'recall',\n",
       " 'recall_macro',\n",
       " 'recall_micro',\n",
       " 'recall_samples',\n",
       " 'recall_weighted',\n",
       " 'roc_auc',\n",
       " 'v_measure_score']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(sklearn.metrics.SCORERS.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from gensim.models.fasttext import FastText\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import WordPunctTokenizer\n",
    "import wikipedia\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "artificial_intelligence = wikipedia.page(\"Artificial Intelligence\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = sent_tokenize(artificial_intelligence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_punctuation_tokenizer = nltk.WordPunctTokenizer()\n",
    "word_tokenized_corpus = [word_punctuation_tokenizer.tokenize(sent) for sent in z]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['human', 'interface', 'computer']\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# from gensim.models import FastText  # FIXME: why does Sphinx dislike this import?\n",
    "from gensim.test.utils import common_texts  # some example sentences\n",
    "\n",
    "print(common_texts[0])\n",
    "print(len(common_texts))\n",
    "\n",
    "model = FastText(size=4, window=3, min_count=1)  # instantiate\n",
    "model.build_vocab(sentences=common_texts)\n",
    "model.train(sentences=common_texts, total_examples=len(common_texts), epochs=10)  # train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_doc = [\"education nationale\",\"gendarmerie nationale\",\"manpower\",\"pomona\",\"starclay\",\"bnp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(list_doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['education', 'nationale'], tags=['0']),\n",
       " TaggedDocument(words=['gendarmerie', 'nationale'], tags=['1']),\n",
       " TaggedDocument(words=['manpower'], tags=['2']),\n",
       " TaggedDocument(words=['pomona'], tags=['3']),\n",
       " TaggedDocument(words=['starclay'], tags=['4']),\n",
       " TaggedDocument(words=['bnp'], tags=['5'])]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 100\n",
    "vec_size = 20\n",
    "alpha = 0.025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/gensim/models/doc2vec.py:319: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    }
   ],
   "source": [
    "model = Doc2Vec(size=vec_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=0.00025,\n",
    "                min_count=1,\n",
    "                dm =1)\n",
    "\n",
    "model.build_vocab(tagged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(max_epochs):\n",
    "    model.train(tagged_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.iter)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"education nationale\",\"gendarmerie nationale\",\"manpower\",\"pomona\",\"starclay\",\"bnp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = tagged_data\n",
    "\n",
    "\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)\n",
    "model.build_vocab(train_corpus)\n",
    "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "ranks = []\n",
    "second_ranks = []\n",
    "for doc_id in range(len(train_corpus)):\n",
    "    inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n",
    "    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "    rank = [docid for docid, sim in sims].index(str(doc_id))\n",
    "    ranks.append(rank)\n",
    "    second_ranks.append(sims[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = []\n",
    "sc.append(model.infer_vector(['France','Telecom']))\n",
    "sc.append(model.infer_vector(['Cuisine','Telecom']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = pd.DataFrame(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0009312409092672169\n",
      "-0.008755287155508995\n"
     ]
    }
   ],
   "source": [
    "# to find vector of doc in training data using tags or in other words, printing the vector of document at index 1 in training data\n",
    "print(model.docvecs['1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1erdoc</td>\n",
       "      <td>1erdoccol1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2èmedoc</td>\n",
       "      <td>2èmedoccol2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0            1\n",
       "0   1erdoc   1erdoccol1\n",
       "1  2èmedoc  2èmedoccol2"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll = [[\"1erdoc\",'1erdoccol1'],[\"2èmedoc\",\"2èmedoccol2\"]]\n",
    "pd.DataFrame(ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 4, 0, 2, 3, 2]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({2: 2, 5: 1, 4: 1, 0: 1, 3: 1})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "counter = collections.Counter(ranks)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [[['1er doc col','2ème doc']]\n",
    "df = pd.DataFrame([{\"lol\":\"bernard tuui\"},{\"lol\":\"MV POON FALL\"},{\"lol\":\"HASTAG DIEZE\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1er doc',\n",
       " '2ème doc',\n",
       " 'bernard tuui',\n",
       " 'MV POON FALL',\n",
       " 'HASTAG DIEZE',\n",
       " 'bernard tuui',\n",
       " 'MV POON FALL',\n",
       " 'HASTAG DIEZE']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.append(df.lol.tolist())\n",
    "flatten(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'finder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-dbdbb73f96bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfinder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_freq_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'finder' is not defined"
     ]
    }
   ],
   "source": [
    "finder.apply_freq_filter(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from multiprocessing import cpu_count\n",
    "num_cores = cpu_count()\n",
    "num_cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-f250cb87c68d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'UNKNOW'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "text = a.fillna('UNKNOW').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-2d752b72e637>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcorpus_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msub\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msub\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "corpus_text = [x.split(\" \") for sub in text for x in sub]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corpus_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-fb8f3d490ec9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbigram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPhrases\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'corpus_text' is not defined"
     ]
    }
   ],
   "source": [
    "bigram = gensim.models.Phrases(corpus_text, min_count=2, threshold=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigram_mod = gensim.models.phrases.Phraser(bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corpus_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-638cd95e4c1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbigram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPhrases\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbigram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpus_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'corpus_text' is not defined"
     ]
    }
   ],
   "source": [
    "bigram = gensim.models.Phrases(corpus_text)\n",
    "result = [bigram[line] for line in corpus_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-496065c74927>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_words_bigrams\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mmake_bigrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "data_words_bigrams  = make_bigrams(a.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-3c7a48bce9ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'UNKNOW'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbigram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "for item in a.fillna('UNKNOW').values:\n",
    "    print(bigram[item])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-14e42fa0fe3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mflat_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "flat_res = [\" \".join(x) for x in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'flat_res' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-746641b70329>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rs_x'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflat_res\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'flat_res' is not defined"
     ]
    }
   ],
   "source": [
    "a['rs_x'] = flat_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'flat_res' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-0dc20f0166e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'flat_res' is not defined"
     ]
    }
   ],
   "source": [
    "len(flat_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import gensim.downloader as api\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from gensim import corpora, matutils, models, similarities\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['calaamar', 'pyjama', 'chateau']]\n",
      "[['pyjama', 'karaté', 'kimono']]\n",
      "[['chateau', 'uruguay']]\n",
      "[['papaye']]\n"
     ]
    }
   ],
   "source": [
    "dataset = [['calaamar','pyjama','chateau'],['pyjama', 'karaté','kimono'],['chateau','uruguay'],['papaye']]\n",
    "\n",
    "dct = Dictionary()\n",
    "corpus = []\n",
    "for line in dataset:\n",
    "    print([line])\n",
    "    dct.add_documents([line])\n",
    "    corpus.append(dct.doc2bow(line))\n",
    "model = TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_sparse_array = matutils.corpus2csc(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1.0\n",
      "  (1, 0)\t1.0\n",
      "  (2, 0)\t1.0\n",
      "  (2, 1)\t1.0\n",
      "  (3, 1)\t1.0\n",
      "  (4, 1)\t1.0\n",
      "  (1, 2)\t1.0\n",
      "  (5, 2)\t1.0\n",
      "  (6, 3)\t1.0\n"
     ]
    }
   ],
   "source": [
    "print(tf_sparse_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "1 0\n",
      "1 2\n",
      "2 0\n",
      "2 1\n",
      "3 1\n",
      "4 1\n",
      "5 2\n",
      "6 3\n"
     ]
    }
   ],
   "source": [
    "row, col = tf_sparse_array.nonzero()\n",
    "for row, col in zip(row, col):\n",
    "    print(str(row) + ' ' + str(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_words(matrix,nword):\n",
    "    count = {}\n",
    "    row, col = tf_sparse_array.nonzero()\n",
    "    for row, col in zip(row, col):\n",
    "        if row in count:\n",
    "            count[row] = count[row] + 1\n",
    "        else:\n",
    "            count[row] = 1\n",
    "    words = list(count.items())\n",
    "    words.sort(key=lambda tup: tup[1],reverse=True)\n",
    "    words = words[0:nword]\n",
    "    words = [i[0] for i in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_n_words(tf_sparse_array,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector2 = model[corpus[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'calaamar': 0,\n",
       " 'chateau': 1,\n",
       " 'pyjama': 2,\n",
       " 'karaté': 3,\n",
       " 'kimono': 4,\n",
       " 'uruguay': 5,\n",
       " 'papaye': 6}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dct.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1)],\n",
       " [(2, 1), (3, 1), (4, 1)],\n",
       " [(1, 1), (5, 1)],\n",
       " [(6, 1)]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dct.token2id['pyjama']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.interfaces.TransformedCorpus at 0x7fb347a2c9b0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector2 = model[corpus]\n",
    "vector2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_cols_name =  {v: k for k, v in dct.token2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1)],\n",
       " [(2, 1), (3, 1), (4, 1)],\n",
       " [(1, 1), (5, 1)],\n",
       " [(6, 1)]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector2.corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.8164965809277261), (1, 0.4082482904638631), (2, 0.4082482904638631)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values(['calaamar', 'chateau', 'pyjama', 'karaté', 'kimono', 'uruguay', 'papaye'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_cols_name.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calaamar\n",
      "chateau\n",
      "pyjama\n",
      "karaté\n",
      "kimono\n",
      "uruguay\n",
      "papaye\n"
     ]
    }
   ],
   "source": [
    "for item in tf_cols_name:\n",
    "    print(tf_cols_name[item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'colss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-3d3249dea3b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'colss' is not defined"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(columns = colss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tf_cols_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_to_sparse_array(matrix,vocab,tops=[],):\n",
    "    row = []\n",
    "    col = []\n",
    "    val = []\n",
    "    if len(tops) != 0:\n",
    "        for index_doc, doc in enumerate(vector2):\n",
    "            for item in doc:\n",
    "                if item[0] in tops:\n",
    "                    col.append(item[0])\n",
    "                    row.append(index_doc)\n",
    "                    val.append(item[1])\n",
    "            A = sparse.csc_matrix((val,(row,col)),shape=(len(vector2.corpus),len(tops)))\n",
    "    else:\n",
    "        for index_doc, doc in enumerate(vector2):\n",
    "            for item in doc:\n",
    "                col.append(item[0])\n",
    "                row.append(index_doc)\n",
    "                val.append(item[1])\n",
    "            A = sparse.csc_matrix((val,(row,col)),shape=(len(vector2.corpus),len(vocab)))\n",
    "   \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.8164965809277261), (1, 0.4082482904638631), (2, 0.4082482904638631)]\n",
      "[(2, 0.3333333333333333), (3, 0.6666666666666666), (4, 0.6666666666666666)]\n",
      "[(1, 0.4472135954999579), (5, 0.8944271909999159)]\n",
      "[(6, 1.0)]\n"
     ]
    }
   ],
   "source": [
    "for item in vector2:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2]\n"
     ]
    }
   ],
   "source": [
    "tops = top_n_words(vector2,2)\n",
    "print(tops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.81649658 0.40824829 0.40824829 0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.33333333 0.66666667 0.66666667 0.\n",
      "  0.        ]\n",
      " [0.         0.4472136  0.         0.         0.         0.89442719\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "sp = tfidf_to_sparse_array(vector2,tf_cols_name)\n",
    "print(sp.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.81649658 0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(sp[:, 0].toarray().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 7)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([0, 1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_cols_name.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.matutils import corpus2dense, corpus2csc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vector2.corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.81649658, 0.        , 0.        , 0.        ],\n",
       "       [0.40824829, 0.        , 0.4472136 , 0.        ],\n",
       "       [0.40824829, 0.33333333, 0.        , 0.        ],\n",
       "       [0.        , 0.66666667, 0.        , 0.        ],\n",
       "       [0.        , 0.66666667, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.89442719, 0.        ],\n",
       "       [0.        , 0.        , 0.        , 1.        ]])"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus2csc(vector2).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparceMatrix_to_csv(origin, spmatrix,vocab,tops=[]):\n",
    "    with open('tmp/dense_tf_idf.csv','w') as f:\n",
    "        wr = csv.writer(f, delimiter=';')\n",
    "        wr.writerow([x for x in vocab.values()])\n",
    "        for index_line in range(0,sp.shape[0]):\n",
    "            wr.writerow(sp[index_line, :].toarray().flatten())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sparceMatrix_to_csv('tmp/test.csv',sp,tf_cols_name,tops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'index_line' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-395-28ffbfcf7d88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex_line\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'index_line' is not defined"
     ]
    }
   ],
   "source": [
    "sp[index_line, :].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "tops = top_n_words(tf_sparse_array,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_to_sparse_array(vector2,tf_cols_name,tops).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "from numpy import array\n",
    "I = array([0,3,1,0])\n",
    "J = array([0,3,1,2])\n",
    "V = array([4,5,7,9])\n",
    "A = sparse.coo_matrix((V,(I,J)),shape=(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import lil_matrix\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from numpy.linalg import solve, norm\n",
    "from numpy.random import rand\n",
    "\n",
    "\n",
    "A = lil_matrix((1000, 1000))\n",
    "A[0, :100] = rand(100)\n",
    "A[1, 100:200] = A[0, :100]\n",
    "A.setdiag(rand(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = A.tocsr()\n",
    "b = rand(1000)\n",
    "x = spsolve(A, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = solve(A.toarray(), b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corpus2csc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-415-d699dd761f3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcorpus2csc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_terms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'corpus2csc' is not defined"
     ]
    }
   ],
   "source": [
    "corpus2csc(vector2, num_terms, num_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.8164965809277261), (1, 0.4082482904638631), (2, 0.4082482904638631)]"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector2.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x2 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 3 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.sparse.csr_matrix([[1, 2], [3, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.8164965809277261), (1, 0.4082482904638631), (2, 0.4082482904638631)]\n",
      "[(2, 0.3333333333333333), (3, 0.6666666666666666), (4, 0.6666666666666666)]\n",
      "[(1, 0.4472135954999579), (5, 0.8944271909999159)]\n",
      "[(6, 1.0)]\n"
     ]
    }
   ],
   "source": [
    "for item in vector2:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {dct.get(id): value for doc in vector2 for id, value in doc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'calaamar': 0.8164965809277261,\n",
       " 'chateau': 0.4472135954999579,\n",
       " 'pyjama': 0.3333333333333333,\n",
       " 'karaté': 0.6666666666666666,\n",
       " 'kimono': 0.6666666666666666,\n",
       " 'uruguay': 0.8944271909999159,\n",
       " 'papaye': 1.0}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.txt') as f:\n",
    "    lines = [line.rstrip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_rs_x = TfidfVectorizer(input='content',max_features=250)\n",
    "vector_rs_x = vect_rs_x.fit_transform(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_cols_name =  {v: k for k, v in vect_rs_x.vocabulary_.items()}\n",
    "df_vector_rs_x = pd.DataFrame(vector_rs_x.toarray())\n",
    "df_vector_rs_x.rename(tf_cols_name,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bateau</th>\n",
       "      <th>chat</th>\n",
       "      <th>chien</th>\n",
       "      <th>je</th>\n",
       "      <th>kimono</th>\n",
       "      <th>loup</th>\n",
       "      <th>pyjama</th>\n",
       "      <th>suis</th>\n",
       "      <th>un</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.57735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bateau  chat  chien       je    kimono  loup    pyjama     suis       un\n",
       "0     0.0   0.0    0.0  0.57735  0.000000   0.0  0.000000  0.57735  0.57735\n",
       "1     0.5   0.5    0.5  0.00000  0.000000   0.5  0.000000  0.00000  0.00000\n",
       "2     0.0   0.0    0.0  0.00000  0.707107   0.0  0.707107  0.00000  0.00000"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vector_rs_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2\n",
       "0  1  2  3\n",
       "1  4  5  6\n",
       "2  7  8  9\n",
       "3  1  2  3\n",
       "4  4  5  6"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = pd.DataFrame([[1,2,3],[4,5,6],[7,8,9],[1,2,3],[4,5,6],[7,8,9],[1,2,3],[4,5,6],[7,8,9]])\n",
    "d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2\n",
       "2  7  8  9\n",
       "3  1  2  3"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[2:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "import os\n",
    "fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': 'http://minio.stable.innovation.insee.eu'})\n",
    "remote_data_dir=\"s3://groupe-1033/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in fs.ls(\"s3://groupe-1033/production/tfidf/\"):\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".keep\n",
      "baseline.py\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "for x in fs.ls(\"s3://groupe-1033/production/tfidf\",refresh=True):\n",
    "    print(os.path.basename(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.touch(\"s3://groupe-1033/production/refined/.keep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC\n",
    "from abc import ABCMeta\n",
    "from abc import abstractmethod\n",
    "\n",
    "from abc import ABC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "import imblearn\n",
    "from imblearn.datasets import fetch_datasets\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from imblearn.ensemble import EasyEnsembleClassifier\n",
    "from imblearn.ensemble import RUSBoostClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from imblearn.metrics import geometric_mean_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "satimage = fetch_datasets()['satimage']\n",
    "X, y = satimage.data, satimage.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6435\n",
      "11618\n"
     ]
    }
   ],
   "source": [
    "oversample = SMOTE()\n",
    "print(len(y))\n",
    "X, y = oversample.fit_resample(X, y)\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11618"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree\n",
      "XGBoost\n",
      "LogRegr\n"
     ]
    }
   ],
   "source": [
    "with open('config.yaml') as f:\n",
    "        configs = yaml.safe_load(f)\n",
    "for model in configs['models']:\n",
    "    for obj in model.keys():\n",
    "        print(model[obj]['type'])\n",
    "        arg = model[obj]['param']\n",
    "#         class_ = getattr(thismodule, model[obj]['type'])\n",
    "#         instance = class_(path_run = path_run,id_run=id_run)\n",
    "#         list_model.append(instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "print(arg)\n",
    "arg['id']= 'dd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import date\n",
    "t = date.today()\n",
    "type(t.isoformat())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
